{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Regression\n",
    "\n",
    "- Consider mean & variance\n",
    "\n",
    "Regression is finding a line that best fits the set of points, where:\n",
    "\n",
    "$$y_1 = A + B_{x1}\\\\\n",
    "y_2 = A + B_{x2}\\\\\n",
    "\\cdots\\\\\n",
    "y_n = A + B_{xn}$$\n",
    "\n",
    "\n",
    "Ideally it would satisfy all equations, however its unlikely that we will so we have to live with some level of error e\n",
    "\n",
    "Hence...\n",
    "\n",
    "### Regression Residuals\n",
    "\n",
    "$$y_1 = A + B_{x1} + e_1\\\\\n",
    "y_2 = A + B_{x2} + e_2\\\\\n",
    "\\cdots\\\\\n",
    "y_n = A + B_{xn} + e_n$$\n",
    "\n",
    "e is technically refered to as the 'regression residuals' of the equation \n",
    "\n",
    "We find the values for A & B such that the magnitude of the residuals are minimized\n",
    "\n",
    "\n",
    "To find the line of \"best fit\" we need to make some assumptions about regression residuals\n",
    "\n",
    "- Have zero mean\n",
    "- common variance\n",
    "- be independet of eachother\n",
    "- be independent of x\n",
    "- be normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Only try to find regression between variables that you have already established a cause and affect relationship between. \n",
    "Otherwise you will be falling fallacy to one of the following two fallacies\n",
    "- Post hoc fallacy - X happens before Y so we conclude that X causes Y\n",
    "- Correlation is causation fallacy - X and Y happen together so we conclude that X causes Y\n",
    "\n",
    "## Quantifying the cause and affect relationship \n",
    "\n",
    "### Independent variable x \n",
    "If X causes Y, then values of x form a vector, called the independent variable or explanatory variable\n",
    "\n",
    "### Dependent variable\n",
    "If X causes Y, then values of y form a vector, called the dependent variable or explained variable\n",
    "\n",
    "### Regression Line\n",
    "The \"best fit\" line whichminimises the sum of the squares of the errors\n",
    "It is the best 'linear unbiased estimatoor'\n",
    "\n",
    "Once we have the regression line we can get the *Fitted values* of Dependent variable\n",
    "\n",
    "#### Fitted values of the dependent variable\n",
    "The fitted line y = A + Bx will yield a different set of values, called the fitted values\n",
    "\n",
    "$$ y' = [Y'_1, Y'_2, Y'_3...y'_n] = A + Bx$$\n",
    "\n",
    "#### Deriving \n",
    "Variannce of the dependent variable can be decomposed into variance of the regression fitted values, and that of the residuals\n",
    "\n",
    "$$ e = y - y' \\\\\n",
    "\\text{becomes}\\\\\n",
    "y = y' + e \\\\ \n",
    "\\text{becomes}\\\\\n",
    "Variance(y) = Variance(y' + e)\n",
    "Variance(y) = Variance(y') Variance(e)$$\n",
    "\n",
    "This allows us to decompose the variance of the dependent variable into the variance of the fitted values and the varaince of the residauls\n",
    "\n",
    "This is known as the Total Variance or Total Sum of Squares (TSS)\n",
    "\n",
    "#### Total Variance\n",
    "A measure of how volatile the dependent variable is, and how much it moves around\n",
    "It can be plugged into our equation\n",
    "\n",
    "$$TSS = Variance(y') + Variance(e)$$\n",
    "\n",
    "#### Explained Variance (ESS)\n",
    "So now we can express TSS as the sum of the values of the fitted values and \n",
    "\n",
    "A measure of how volatile the fitted values are - these ccome from the regresion line TSS = Variance(y)\n",
    "\n",
    "*The better the fit of the regression line the more closely ESS will match TSS*\n",
    "\n",
    "#### Residual Variance (RSS)\n",
    "The difference between ESS & TSS is the Variance of the Residuals\n",
    "\n",
    "This is known as the Residual Variance or Residual Sum of Squares (RSS)\n",
    "\n",
    "This symbolises the variance in the dependent variable that can not be explained by the regression\n",
    "\n",
    "Pluggin this in we can decompose the equation to...\n",
    "\n",
    "$$TSS = ESS + RSS$$\n",
    "\n",
    "*The more variance we are able to explain, the more closely ESS will approach TSS*\n",
    "\n",
    "\n",
    "$$ R{^2} \\\\\n",
    "\\text{The percentage of total variance explained by the regression.} \\\\\n",
    "\\text{Usually, the higher the} R^2 \\text{ the better the quality of the regression (upper bound is 100\\%)} $$\n",
    "\n",
    "The R squared tells us how much of the variation in one data series is caused by the variation in another data series\n",
    "\n",
    "\n",
    "\n",
    "## R squared as variance\n",
    "$$ R{^2} \\text{as Variance} $$\n",
    "\n",
    "the fitted line y = a + bx will yield a different set of values, called the fitted values\n",
    "\n",
    "The higher the R^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
